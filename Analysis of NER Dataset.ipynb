{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_PATH = \"./cache\"\n",
    "\n",
    "TRAIN_CACHE_FN = \"ner-train-data.cache\"\n",
    "VALID_CACHE_FN = \"ner-valid-data.cache\"\n",
    "TEST_CACHE_FN = \"ner-test-data.cache\"\n",
    "\n",
    "LABEL_FN = \"./vocab/ner.label\"\n",
    "\n",
    "MAX_SEQ_LEN = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LABEL_FN, \"rb\") as fp:\n",
    "    data = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of labels 15\n",
      "Label List :  ['CV', 'TM', 'OG', 'FD', 'AM', 'LC', 'QT', 'MT', 'DT', 'PT', 'TI', 'AF', 'EV', 'TR', 'PS']\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "\n",
    "for key, _ in data[\"l2i\"].items():\n",
    "    if \"-\" in key:\n",
    "        label = \"\".join(key.split(\"-\")[1:])\n",
    "\n",
    "        labels.append(label)\n",
    "    \n",
    "labels = list(set(labels))\n",
    "\n",
    "print(\"Length of labels\", len(labels))\n",
    "print(\"Label List : \", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current file : ner-train-data.cache\n",
      "Length of tokens :  298921\n",
      "Max Length :  198\n",
      "Min Length :  0\n",
      "Average Length :  17.701459582966738\n",
      "Over 200 length :  0\n",
      "Frequency of Each Labels :  {'LC': 91211, 'O': 4102262, 'OG': 178455, 'AF': 68222, 'CV': 246078, 'QT': 182911, 'TM': 47786, 'PS': 141356, 'DT': 133658, 'FD': 12209, 'AM': 12136, 'EV': 39768, 'TI': 17184, 'MT': 7814, 'TR': 7061, 'PT': 3227}\n",
      "\n",
      "Current file : ner-valid-data.cache\n",
      "Length of tokens :  37357\n",
      "Max Length :  241\n",
      "Min Length :  0\n",
      "Average Length :  17.652514923575232\n",
      "Over 200 length :  2\n",
      "Frequency of Each Labels :  {'O': 511822, 'DT': 16677, 'OG': 22509, 'CV': 30699, 'AM': 1420, 'PS': 17612, 'AF': 8111, 'TI': 2052, 'LC': 11374, 'EV': 5065, 'TM': 6180, 'QT': 22122, 'PT': 402, 'FD': 1560, 'MT': 994, 'TR': 846}\n",
      "\n",
      "Current file : ner-test-data.cache\n",
      "Length of tokens :  37358\n",
      "Max Length :  151\n",
      "Min Length :  0\n",
      "Average Length :  17.69492478184057\n",
      "Over 200 length :  0\n",
      "Frequency of Each Labels :  {'O': 515050, 'CV': 30694, 'LC': 11212, 'OG': 21670, 'PS': 17241, 'QT': 22927, 'DT': 16293, 'AM': 1539, 'TR': 770, 'AF': 8093, 'TI': 1936, 'MT': 958, 'TM': 5691, 'EV': 4816, 'FD': 1703, 'PT': 454}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for cache_fn in [TRAIN_CACHE_FN, VALID_CACHE_FN, TEST_CACHE_FN]:\n",
    "    \n",
    "    print(\"Current file : \" + cache_fn)\n",
    "    \n",
    "    with open(os.path.join(CACHE_PATH, cache_fn), \"rb\") as fp:\n",
    "        data = pickle.load(fp)\n",
    "\n",
    "    tokens, labels = data[\"tokens\"], data[\"labels\"]\n",
    "    \n",
    "    print(\"Length of tokens : \", len(tokens))\n",
    "    exceed_seq_len = 0\n",
    "    max_seq_len = 0\n",
    "    min_seq_len = 10000\n",
    "    seq_len_buffer = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if len(token) > MAX_SEQ_LEN:\n",
    "            exceed_seq_len += 1\n",
    "\n",
    "        if len(token) > max_seq_len:\n",
    "            max_seq_len = len(token)\n",
    "\n",
    "        if len(token) < min_seq_len:\n",
    "            min_seq_len = len(token)\n",
    "\n",
    "        seq_len_buffer.append(len(token))\n",
    "\n",
    "\n",
    "    print(\"Max Length : \", max_seq_len)\n",
    "    print(\"Min Length : \", min_seq_len)\n",
    "    print(\"Average Length : \", sum(seq_len_buffer) / len(seq_len_buffer))\n",
    "    \n",
    "    over_200 = 0\n",
    "\n",
    "    for token in tokens:\n",
    "        if len(token) > 200:\n",
    "            over_200 += 1\n",
    "\n",
    "    print(\"Over 200 length : \", over_200)\n",
    "    \n",
    "    \n",
    "    label_dict = {}\n",
    "    \n",
    "    for label in labels:\n",
    "        for l in label:\n",
    "            if \"-\" in l:\n",
    "                l = \"\".join(l.split(\"-\")[1:])\n",
    "\n",
    "            if l not in label_dict:\n",
    "                label_dict.setdefault(l, 0)\n",
    "\n",
    "            label_dict[l] += 1\n",
    "            \n",
    "    print(\"Frequency of Each Labels : \", label_dict)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
