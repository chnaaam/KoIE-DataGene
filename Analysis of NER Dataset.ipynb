{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_PATH = \"./cache\"\n",
    "\n",
    "TRAIN_CACHE_FN = \"train-ner-koelectra-data.cache\"\n",
    "VALID_CACHE_FN = \"valid-ner-koelectra-data.cache\"\n",
    "TEST_CACHE_FN = \"test-ner-koelectra-data.cache\"\n",
    "\n",
    "LABEL_FN = \"./label/ner.label\"\n",
    "\n",
    "MAX_SEQ_LEN = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LABEL_FN, \"rb\") as fp:\n",
    "    data = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of labels 15\n",
      "Label List :  ['EV', 'OG', 'AM', 'FD', 'TM', 'TI', 'TR', 'CV', 'MT', 'LC', 'PT', 'AF', 'DT', 'PS', 'QT']\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "\n",
    "for key, _ in data[\"l2i\"].items():\n",
    "    if \"-\" in key:\n",
    "        label = \"\".join(key.split(\"-\")[1:])\n",
    "\n",
    "        labels.append(label)\n",
    "    \n",
    "labels = list(set(labels))\n",
    "\n",
    "print(\"Length of labels\", len(labels))\n",
    "print(\"Label List : \", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current file : train-ner-koelectra-data.cache\n",
      "211\n",
      "833 164251\n",
      "Current file : valid-ner-koelectra-data.cache\n",
      "183\n",
      "102 20528\n",
      "Current file : test-ner-koelectra-data.cache\n",
      "200\n",
      "101 20545\n"
     ]
    }
   ],
   "source": [
    "for cache_fn in [TRAIN_CACHE_FN, VALID_CACHE_FN, TEST_CACHE_FN]:\n",
    "    \n",
    "    print(\"Current file : \" + cache_fn)\n",
    "    \n",
    "    with open(os.path.join(CACHE_PATH, cache_fn), \"rb\") as fp:\n",
    "        data_list = pickle.load(fp)\n",
    "    \n",
    "    len_tokens, len_labels = [], []\n",
    "    unk_count = 0\n",
    "    \n",
    "    for data in data_list:\n",
    "        tokens, labels = data[\"tokens\"], data[\"labels\"]\n",
    "        \n",
    "        len_tokens.append(len(tokens))\n",
    "        len_labels.append(len(labels))\n",
    "        \n",
    "        if \"[UNK]\" in tokens:\n",
    "            unk_count += 1\n",
    "        \n",
    "    print(max(len_tokens))\n",
    "    print(unk_count, len(len_tokens))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "electra_tokenizer = AutoTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "electra_tokenizer.cls_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 11229, 29173, 13352, 25541, 4110, 7824, 17788, 18, 3]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "electra_tokenizer.convert_tokens_to_ids(['[CLS]', '한국어', 'EL', '##EC', '##TRA', '##를', '공유', '##합니다', '.', '[SEP]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_tokenizer.cls_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 9187, 23565, 10990, 20707, 2138, 5194, 11800, 18, 2]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_tokenizer.convert_tokens_to_ids(['[CLS]', '한국어', 'EL', '##EC', '##TRA', '##를', '공유', '##합니다', '.', '[SEP]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
