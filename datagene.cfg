use_cuda: True

path:
    root: ./
    model: model
    cache: cache
    vocab: vocab

# Model Format
# - Our framework recognizes the model layer separately using "-".
#
# Current Defined Layer List(layer name, tokenizer name[optional])
# - bert(monologg/kobert), bert
# - electra(monologg/koelectra-base-v3-discriminator), electra
# - roberta(klue/roberta-base), roberta
# - crf
#
# Example
#     {layer1}-{layer2}-{layer3}-...

tasks:
    srl:
        model_name: bert
        model_type: sequence-labeling
        tokenizer: bert
        dataset:
            path: ./data/srl
            data: NXSR1902111171.json
            train: train.json
            valid: valid.json
            test: test.json


parameters:
    epochs: 3
    batch_size: 24
    num_workers: 0
    max_seq_len: 256
    
    criterion: cross-entropy
    optimizer: adamw
    scheduler: lambda

    learning_rate: 1e-5
    crf_in_features: 768
    weight_decay: 0.01

    fp16: True

    # It is a parameter used to retrieve model name that has been pretrained model.
    model_name: monologg/kobert